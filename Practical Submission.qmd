---
title: "Parallel Computing Prac Solutions"
author: "Tshwanelo Nkalanga"
format: html
---

```{r}
#| echo: false
#| warning: false
#| message: false

library(foreach)
library(doParallel)
library(iterators)
library(MASS)
library(parallel)
library(iterators)
library(doParallel)
```

## Question 1: Exponential Distribution Simulation

```{r}

num_simulations <- 100
set.seed(123)

results <- foreach(i = 1:num_simulations, .combine = rbind) %do% {
    sample_data <- rexp(100, rate = 1)
    c(mean(sample_data), var(sample_data))
}

results_df <- data.frame(
    Observation = paste("Simulation", 1:num_simulations),
    Mean = results[,1],
    Variance = results[,2]
)

knitr::kable(head(results_df), digits = 4, caption = "First 6 Simulation Results")
```

The simulation generates 100 exponential samples per iteration, calculating mean and variance. The results demonstrate the law of large numbers - as sample size increases, means approach true mean (1) while variances stabilize around theoretical variance (1). Parallelization isn't needed here due to small computational load per iteration.

## Question 2: Bootstrapping Galaxy Medians

```{r}
data(galaxies)
B <- 10000
num_cores <- detectCores() - 1

# Serial processing
serial_time <- system.time({
    serial_med <- replicate(B, median(sample(galaxies, replace = TRUE)))
})[3]

# Parallel processing (chunked)
cl <- makeCluster(num_cores)
registerDoParallel(cl)

chunk_time <- system.time({
    chunk_med <- foreach(i = 1:10, .combine = c) %dopar% {
        replicate(1000, median(sample(galaxies, replace = TRUE)))
    }
})[3]

stopCluster(cl)

timing_comparison <- data.frame(
    Method = c("Serial", "Parallel (Chunked)"),
    Time = c(serial_time, chunk_time)
)

knitr::kable(timing_comparison, caption = "Processing Time Comparison (seconds)")
```

Chunked parallel processing (1000 samples/iteration) shows significant speedup (0.07s vs 0.78s serial) by reducing parallelization overhead. This demonstrates that grouping computations minimizes data transfer costs between cores, making parallelization effective for medium-sized tasks.
